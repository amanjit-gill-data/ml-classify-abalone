---
title: "Classification and Pricing of Farmed Abalone"
author: "Amanjit Gill"
date: "2022-12-05"
output: 
    html_notebook:
        theme: flatly
---

```{css echo=FALSE}

h1, h2, h3 {
    font-weight: bold;
}

h1.title {
    font-size: 2em;
}

h2 {
    font-size: 1.5em;
}

h3 {
    font-size: 1.2em;
}

```


## 1. Exploratory Analysis

```{r setup, message=FALSE}

library(here)
library(dplyr)
library(MVN)
library(heplots)
library(MASS)
library(e1071)

par(
    family = "serif",
    ps = 11,
    mar = c(4.5, 1, 2, 1)
)

params <- par(no.readonly = TRUE)

abalone <- read.csv("abalone.csv")

# list of just the numerical variables
cols <- colnames(abalone)[-1]
```

### 1.1 Summary statistics

Statistical summary for each numerical variable.
```{r summary}

summary(abalone[cols])

```

RESULTS:

none of the maximum values appear to be out of range, according to sources. however:

min height = 0 (impossible).

find out how many there are, compared to how many cases there are in total.
```{r}
abalone %>% filter(Height == 0) %>% nrow()
nrow(abalone)
```

only 2 out of 4177. not a serious problem with the data. remove.
```{r}
abalone <- abalone %>% filter(Height > 0)
```


all the minimum weights are between 0.1 and 0.4 grams. not sure how realistic it is to catch and measure an abalone that tiny. may be outlier errors, or may be real.

Check for missing values.
```{r missing values}

nrow(abalone)
sum(complete.cases(abalone))
```

RESULT: No missing values; all rows are complete.

### 1.2 Univariate outliers

```{r outliers}

boxplots <- function(data, colnames, pars, outfile) {
    pdf(outfile)
    par(pars)
    gridrows <- ceiling(length(colnames)/2)
    par("mfrow"=c(gridrows, 2))

    outliers <- c()
    
    for (c in colnames) {
        b <- boxplot(data[,c], horizontal = TRUE, xlab = c)
        outliers <- c(outliers, length(b$out))
    }
    
    return(data.frame(outliers, row.names = colnames))
}

outliers <- boxplots(abalone, cols, params, "1.2.pdf")

```

RESULTS: 

every numerical variable has some outliers.
however, for 7 of them, the number of outliers is between 0.5 and 1.5% of the sample size, so it's not clear that they will affect the analysis.
for the 8th, Rings, there are 278 outliers, 6.7% of the sample size. This may be impactful, and should be confirmed as being so.

we will not remedy the outliers here. we will try transformations first, to see if some of outliers have been pulled in.

### 1.3 Linearity and homoscedasticity

```{r linearity}
pdf("1.3.pdf")
par(params)

pairs(abalone[cols])
```

RESULTS:

non-linear between all the weights and the length and diameter

heteroscedasticity between rings and everything except for height

oval shape implies univariate normality of whole, shucked and viscera weights
others are too hard to tell


### 1.4 Normality

```{r normality}

density_charts <- function(data, colnames, pars, outfile) {
    pdf(outfile)
    par(pars)
    gridrows <- ceiling(length(colnames)/2)
    par("mfrow"=c(gridrows, 2))

    for (c in colnames) {
        plot(density(data[,c]), xlab=c)
    }
}

density_charts(abalone, cols, params, "1.4.pdf")

```

Mardia's skewness and kurtosis test
```{r mardia}

mvn_test <- mvn(abalone[cols], mvnTest = "mardia", univariateTest = "SW")

```

### 1.5 Transformations

moderately different from normal: square root

substantially different from normal: logarithm

extremely different from normal: inverse


apply following transformations:

|Variable|Transformation|
|:--------|:--------------|
|Length|Reflect and square root|
|Diameter|Reflect and square root|
|Height|Logarithm|
|Whole weight|Square root|
|Shucked weight|Square root|
|Viscera weight|Square root|
|Shell weight|Square root|
|Rings|Square root|


```{r}

reflect <- function(vals) {
    maxval <- max(vals)
    maxval+1 - vals
}


abalone.t <- abalone %>% mutate(
    Length = sqrt(reflect(Length)),
    Diameter = sqrt(reflect(Diameter)),
    Height = log(Height),
    Whole.weight = sqrt(Whole.weight),
    Shucked.weight = sqrt(Shucked.weight),
    Viscera.weight = sqrt(Viscera.weight),
    Shell.weight = sqrt(Shell.weight),
    Rings = sqrt(Rings),
    .keep = "unused"
)

```

recheck normality
```{r}

density_charts(abalone.t, cols, params, "1.5.pdf")
mvn_test.t <- mvn(abalone.t[cols], mvnTest = "mardia", univariateTest = "SW")
```

still fails all tests, but this is because the tests are more likely to fail at larger N for small deviations. visually, they look mostly normal, which matters more than the hyp test.

much improved skewness - very small compared to original data

for kurtosis, most variables have gone from slightly positive (too peaked) to slightly negative (too flat), but the magnitudes are 1 or below, with the exception of Height, which has experienced a marked improvement (from 76.62 to 4.57).

kurtosis generally does not affect estimates of variance for large sample sizes, so these transformations are considered acceptable.

table comparing original skewness and new skewness
```{r}
skew <- data.frame(mvn_test$Descriptives$Skew, mvn_test.t$Descriptives$Skew)
skew[,"Ratio"] <- abs(skew[,2]/skew[,1])
skew
```

table comparing original kurtosis and new kurtosis
```{r}
kurt <- data.frame(mvn_test$Descriptives$Kurtosis, mvn_test.t$Descriptives$Kurtosis)
kurt[,"Ratio"] <- abs(kurt[,2]/kurt[,1])
kurt
```


### 1.6 Removal of outliers

first recheck how many outliers remain after transformation.

```{r}
outliers.t <- boxplots(abalone.t, cols, params, "1.6.pdf")
data.frame(outliers, outliers.t)
```

RESULTS:

new boxplots show far less congestion of outliers at the extremities, except for Height. this is confirmed by the comparison table, which shows a remarkable reduction in the number of outliers for every transformed variable, except, as expected, for Height. For all variables bar Height and Rings, the number of outliers is now single digits, meaning the transformations have allowed us to salvage more data points for the upcoming prediction activities.

remove remaining outliers.
```{r}
for (c in cols) {
    outs <- boxplot.stats(abalone.t[,c])$out
    abalone.t <- abalone.t %>% filter(!(abalone.t[,c] %in% outs))
}
```

### 1.7 Check

final reexamination to confirm achievement of aims.
```{r}
density_charts(abalone.t, cols, params, "1.7.pdf")

sk.final <- mvn(abalone.t[cols], mvnTest = "mardia", univariateTest = "SW")$Descriptives %>% select(c(Skew, Kurtosis))
sk.final
```

RESULT: skewness and kurtosis are now all now below a magnitude of 1.

despite failure of SW test, can now be confident of univariate normality.

## 2. Predicting Abalone Sex

first convert response variable to factor
```{r}
abalone.t <- abalone.t %>% mutate(Sex=factor(Sex, levels=c("I", "F", "M")))
```


2.1. Check equal covariances assumption.

```{r}

predictors = c("Length", "Diameter", "Height")
boxM(abalone.t[,predictors], abalone.t[,"Sex"])

```
RESULT: Highly significant. Covariances are NOT equal. Therefore, start with QDA, not LDA.

2.2 Quadratic discriminant analysis

```{r}
cm.qda <- table(
    truth=abalone.t$Sex, 
    prediction=qda(Sex ~ Length+Diameter+Height, data=abalone.t, CV=TRUE)$class
)

accuracy.qda <- sum(diag(cm.qda))/sum(cm.qda)
accuracy.qda
```
RESULT: Only half of the cases were predicted correctly.

2.3 Linear discriminant analysis

```{r}
cm.lda <- table(
    truth=abalone.t$Sex,
    prediction=lda(Sex ~ Length+Diameter+Height, data=abalone.t, CV=TRUE)$class
)

accuracy.lda <- sum(diag(cm.lda))/sum(cm.lda)
accuracy.lda
```

RESULT: LDA performs slightly better. Examine classes for which it performs well:
```{r}
cm.lda

accuracy.lda.I <- cm.lda["I","I"]/sum(cm.lda["I",])
accuracy.lda.F <- cm.lda["F","F"]/sum(cm.lda["F",])
accuracy.lda.M <- cm.lda["M","M"]/sum(cm.lda["M",])

```

RESULT: Very poor performance for predicting females. Four-fifths of females will be classified incorrectly. 

Approximately two-thirds correct predictions for infants and males. 

2.4 Support vector machine with linear kernel

```{r}
svm.lin <- tune.svm(
    Sex ~ Length+Diameter+Height, 
    data=abalone.t,
    kernel="linear",
    cost=c(0.1, 1, 5, 10)
)

accuracy.svm.lin <- 1 - svm.lin$best.performance
accuracy.svm.lin
```

RESULT: Not better than LDA.

2.5 Support vector machine with radial kernel

```{r}

svm.rad <- tune.svm(
    Sex ~ Length+Diameter+Height, 
    data=abalone.t,
    kernel="radial",
    gamma=c(0.1, 1, 5, 10),
    cost=c(0.1, 1, 5, 10)
)

accuracy.svm.rad <- 1 - svm.rad$best.performance
accuracy.svm.rad

svm.rad$best.parameters
```

RESULT: About the same as LDA. Slightly better than linear kernel.

compute accuracy of radial SVM.





```{r}
mod <- svm(
        Sex ~ Length+Diameter+Height, 
        data=abalone.t,
        kernel="radial",
        gamma=10,
        cost=1,
    )

plot(mod, data=abalone.t, Length~Diameter)

```



